{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPh382eJ5DLcc2XiDvU53GH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hritiksth764/END-COURSE-SESSION-4.0-/blob/main/Session5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8OCRetzDKjj"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "from torchtext.datasets import DBpedia, YelpReviewPolarity, SogouNews, AG_NEWS\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "\n",
        "#Class 1\n",
        "class Classification(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "      super(Classification, self).__init__()\n",
        "      self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "      self.fc = nn.Linear(embed_dim, num_class)\n",
        "      self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "      initrange = 0.5\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "      self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "      self.fc.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "      embedded = self.embedding(text, offsets)\n",
        "      return self.fc(embedded)\n",
        "\n",
        "#Class 2\n",
        "class MainClass:  \n",
        "  def __init__(self, dataset_class=AG_NEWS):\n",
        "    self.model = None\n",
        "    self.dataset_class = dataset_class\n",
        "\n",
        "    train_iter = self.dataset_class(split = 'train')\n",
        "    self.vocab = build_vocab_from_iterator(self.yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "    self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "\n",
        "    self.text_pipeline = lambda x: self.vocab(tokenizer(x))\n",
        "    self.label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "  @staticmethod\n",
        "  def yield_tokens(data_iter):\n",
        "      for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "  def train(self, dataloader, epoch):\n",
        "      self.model.train()\n",
        "      total_acc, total_count = 0, 0\n",
        "      log_interval = 500\n",
        "      start_time = time.time()\n",
        "\n",
        "      for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "          self.optimizer.zero_grad()\n",
        "          predited_label = self.model(text, offsets)\n",
        "          loss = self.criterion(predited_label, label)\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
        "          self.optimizer.step()\n",
        "          total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "          total_count += label.size(0)\n",
        "          if idx % log_interval == 0 and idx > 0:\n",
        "              elapsed = time.time() - start_time\n",
        "              print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                    '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                                total_acc/total_count))\n",
        "              total_acc, total_count = 0, 0\n",
        "              start_time = time.time()\n",
        "\n",
        "  def evaluate(self, dataloader):\n",
        "      self.model.eval()\n",
        "      total_acc, total_count = 0, 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "              predited_label = self.model(text, offsets)\n",
        "              loss = self.criterion(predited_label, label)\n",
        "              total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "              total_count += label.size(0)\n",
        "      return total_acc/total_count\n",
        "\n",
        "\n",
        "  def collate_batch(self, batch):\n",
        "      label_list, text_list, offsets = [], [], [0]\n",
        "\n",
        "      for (_label, _text) in batch:\n",
        "          label_list.append(self.label_pipeline(_label))\n",
        "          processed_text = torch.tensor(self.text_pipeline(_text), dtype=torch.int64)\n",
        "          text_list.append(processed_text)\n",
        "          offsets.append(processed_text.size(0))\n",
        "\n",
        "      label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "      offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "      text_list = torch.cat(text_list)\n",
        "\n",
        "      return label_list.to(device), text_list.to(device), offsets.to(device)   \n",
        "\n",
        "\n",
        "  def train_and_validate(self):\n",
        "    train_iter = self.dataset_class(split='train')\n",
        "    num_class = len(set([label for (label, text) in train_iter]))\n",
        "    vocab_size = len(self.vocab)\n",
        "    emsize = 64\n",
        "    self.model = Classification(vocab_size, emsize, num_class).to(device)\n",
        "      \n",
        "    self.criterion = torch.nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1.0, gamma=0.1)\n",
        "\n",
        "    total_accu = None\n",
        "    train_iter, test_iter = self.dataset_class()\n",
        "    train_dataset = to_map_style_dataset(train_iter)\n",
        "    test_dataset = to_map_style_dataset(test_iter)\n",
        "    num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "    split_train_, split_valid_ = \\\n",
        "        random_split(train_dataset, \n",
        "                    [num_train, len(train_dataset) - num_train]\n",
        "                    )\n",
        "\n",
        "    train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                                  shuffle=True, collate_fn=self.collate_batch)\n",
        "    valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                                  shuffle=True, collate_fn=self.collate_batch)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=self.collate_batch)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        self.train(train_dataloader, epoch)\n",
        "        accu_val = self.evaluate(valid_dataloader)\n",
        "        if total_accu is not None and total_accu > accu_val:\n",
        "          scheduler.step()\n",
        "        else:\n",
        "          total_accu = accu_val\n",
        "        print('-' * 59)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "              'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                              time.time() - epoch_start_time,\n",
        "                                              accu_val))\n",
        "        print('-' * 59)\n",
        "\n",
        "    print('Checking the results of test dataset.')\n",
        "    accu_test = self.evaluate(test_dataloader)\n",
        "    print('test accuracy {:8.3f}'.format(accu_test))\n",
        "  \n",
        "  def predict(self, text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = self.model(text, torch.tensor([0]))\n",
        "        print(output)\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "  def predict_ouput(self, input_text, labels_dict):\n",
        "    self.model = self.model.to(\"cpu\")\n",
        "    print(\"This is a %s news\" %labels_dict[self.predict(input_text, self.text_pipeline)])\n",
        "\n",
        "\n",
        "main_class = MainClass(SogouNews)\n",
        "main_class.train_and_validate()\n",
        "main_class.predict_ouput(\n",
        "    input_text = 'su4 du4 : ( shuo1 mi2ng : dia3n ji1 zi4 do4ng bo1 fa4ng )\\n shuo1 mi2ng : dia3n ji1 ga1i a4n niu3 , xua3n ze2 yi1 lu4n ta2n ji2 ke3 ',\n",
        "    labels_dict = {\n",
        "      1: 'Sports',\n",
        "      2: 'Finance',\n",
        "      3: 'Entertainment',\n",
        "      4: 'Automobile',\n",
        "      5: 'Technology'}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBVOsorSFjx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2ecdcc-e183-4d49-ec36-7c4a00cf120b"
      },
      "source": [
        "main_class = MainClass(DBpedia)\n",
        "main_class.train_and_validate()\n",
        "main_class.predict_ouput(\n",
        "    input_text = 'Brekke Church (Norwegian: Brekke kyrkje is a parish church in Gulen Municipality in Sogn og Fjordane county, Norway. It is located in the village of Brekke. The church is part of the Brekke parish in the Nordhordland deanery in the Diocese of BjÃ¸rgvin. The white, wooden church, which has 390 seats, was consecrated on 19 November 1862 by the local Dean Thomas Erichsen. The architect Christian Henrik Grosch made the designs for the church, which is the third church on the site.',\n",
        "    labels_dict = {\n",
        "                0: 'Company',\n",
        "                1: 'EducationalInstitution',\n",
        "                2: 'Artist',\n",
        "                3: 'Athlete',\n",
        "                4: 'OfficeHolder',\n",
        "                5: 'MeanOfTransportation',\n",
        "                6: 'Building',\n",
        "                7: 'NaturalPlace',\n",
        "                8: 'Village',\n",
        "                9: 'Animal',\n",
        "                10: 'Plant',\n",
        "                11: 'Album',\n",
        "                12: 'Film',\n",
        "                13: 'WrittenWork'}\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68.3M/68.3M [00:00<00:00, 125MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.694\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.909\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.936\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.951\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.957\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.961\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.965\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.967\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.968\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.970\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.970\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.974\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.973\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.973\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.973\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.975\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 79.90s | valid accuracy    0.972 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.979\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.978\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.982\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.981\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 79.54s | valid accuracy    0.978 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.984\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.984\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.984\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.985\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 79.26s | valid accuracy    0.978 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.987\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.988\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.988\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 79.42s | valid accuracy    0.979 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.989\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.990\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 79.35s | valid accuracy    0.980 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.980\n",
            "tensor([[ 1.9328,  3.4267, -1.9950, -8.7373, -2.2327, -1.7993, 12.3401,  1.6043,\n",
            "          5.9967, -5.2832, -5.0450, -0.6517, -1.1392,  1.9472]])\n",
            "This is a NaturalPlace news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrx8IvJr-29",
        "outputId": "2dca6cb9-90bf-481f-8f1c-e34dfba46c73"
      },
      "source": [
        "main_class = MainClass(SogouNews)\n",
        "main_class.train_and_validate()\n",
        "main_class.predict_ouput(\n",
        "    input_text = 'su4 du4 : ( shuo1 mi2ng : dia3n ji1 zi4 do4ng bo1 fa4ng )\\n shuo1 mi2ng : dia3n ji1 ga1i a4n niu3 , xua3n ze2 yi1 lu4n ta2n ji2 ke3 ',\n",
        "    labels_dict = {\n",
        "      1: 'Sports',\n",
        "      2: 'Finance',\n",
        "      3: 'Entertainment',\n",
        "      4: 'Automobile',\n",
        "      5: 'Technology'}\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 6680 batches | accuracy    0.816\n",
            "| epoch   1 |  1000/ 6680 batches | accuracy    0.910\n",
            "| epoch   1 |  1500/ 6680 batches | accuracy    0.918\n",
            "| epoch   1 |  2000/ 6680 batches | accuracy    0.916\n",
            "| epoch   1 |  2500/ 6680 batches | accuracy    0.926\n",
            "| epoch   1 |  3000/ 6680 batches | accuracy    0.925\n",
            "| epoch   1 |  3500/ 6680 batches | accuracy    0.924\n",
            "| epoch   1 |  4000/ 6680 batches | accuracy    0.924\n",
            "| epoch   1 |  4500/ 6680 batches | accuracy    0.929\n",
            "| epoch   1 |  5000/ 6680 batches | accuracy    0.928\n",
            "| epoch   1 |  5500/ 6680 batches | accuracy    0.929\n",
            "| epoch   1 |  6000/ 6680 batches | accuracy    0.927\n",
            "| epoch   1 |  6500/ 6680 batches | accuracy    0.927\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 311.26s | valid accuracy    0.927 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  1000/ 6680 batches | accuracy    0.929\n",
            "| epoch   2 |  1500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  2000/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  2500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  3000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  3500/ 6680 batches | accuracy    0.933\n",
            "| epoch   2 |  4000/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  4500/ 6680 batches | accuracy    0.929\n",
            "| epoch   2 |  5000/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  5500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  6000/ 6680 batches | accuracy    0.933\n",
            "| epoch   2 |  6500/ 6680 batches | accuracy    0.933\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 311.04s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  1000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  1500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  2000/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  2500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  3000/ 6680 batches | accuracy    0.930\n",
            "| epoch   3 |  3500/ 6680 batches | accuracy    0.932\n",
            "| epoch   3 |  4000/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  4500/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  5000/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  5500/ 6680 batches | accuracy    0.936\n",
            "| epoch   3 |  6000/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  6500/ 6680 batches | accuracy    0.936\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 315.60s | valid accuracy    0.936 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 6680 batches | accuracy    0.938\n",
            "| epoch   4 |  1000/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  1500/ 6680 batches | accuracy    0.934\n",
            "| epoch   4 |  2000/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  2500/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  3000/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  3500/ 6680 batches | accuracy    0.933\n",
            "| epoch   4 |  4000/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  4500/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  5000/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  5500/ 6680 batches | accuracy    0.932\n",
            "| epoch   4 |  6000/ 6680 batches | accuracy    0.934\n",
            "| epoch   4 |  6500/ 6680 batches | accuracy    0.937\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 314.67s | valid accuracy    0.937 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 6680 batches | accuracy    0.937\n",
            "| epoch   5 |  1000/ 6680 batches | accuracy    0.935\n",
            "| epoch   5 |  1500/ 6680 batches | accuracy    0.938\n",
            "| epoch   5 |  2000/ 6680 batches | accuracy    0.938\n",
            "| epoch   5 |  2500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  3000/ 6680 batches | accuracy    0.938\n",
            "| epoch   5 |  3500/ 6680 batches | accuracy    0.935\n",
            "| epoch   5 |  4000/ 6680 batches | accuracy    0.935\n",
            "| epoch   5 |  4500/ 6680 batches | accuracy    0.937\n",
            "| epoch   5 |  5000/ 6680 batches | accuracy    0.936\n",
            "| epoch   5 |  5500/ 6680 batches | accuracy    0.937\n",
            "| epoch   5 |  6000/ 6680 batches | accuracy    0.934\n",
            "| epoch   5 |  6500/ 6680 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 313.84s | valid accuracy    0.937 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.934\n",
            "tensor([[ 4.8418, -0.3598, -3.1530,  1.1293, -2.3471]])\n",
            "This is a Sports news\n"
          ]
        }
      ]
    }
  ]
}
